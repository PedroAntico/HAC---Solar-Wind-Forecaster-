#!/usr/bin/env python3
"""
hac_v6_predictor_github.py - PREDICTOR OTIMIZADO PARA GITHUB FREE

Caracter√≠sticas principais:
- Baixo consumo de mem√≥ria (float32, garbage collection)
- Fallbacks autom√°ticos para scalers ausentes
- Valida√ß√£o f√≠sica das previs√µes
- Logging detalhado para debug
- Singleton pattern para evitar m√∫ltiplas inst√¢ncias
"""

import os
import re
import json
import logging
import gc
import warnings
from typing import Dict, Any, Tuple, Optional, List
from dataclasses import dataclass

import numpy as np
import pandas as pd

# Configura√ß√£o otimizada para GitHub Free
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Silencia TF
warnings.filterwarnings('ignore')

# Logging otimizado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

try:
    from tensorflow.keras.models import load_model
    TF_AVAILABLE = True
except ImportError as e:
    logger.warning(f"TensorFlow n√£o dispon√≠vel: {e}")
    TF_AVAILABLE = False

try:
    from hac_v6_config import HACConfig
except ImportError:
    # Fallback para import local
    import sys
    sys.path.append('.')
    from hac_v6_config import HACConfig


@dataclass
class PredictionResult:
    """Resultado estruturado de previs√£o"""
    values: Dict[str, float]
    horizon: int
    valid: bool
    warnings: List[str]
    physical_checks: Dict[str, bool]


class HACv6PredictorGithub:
    """Predictor otimizado para GitHub Free com fallbacks robustos"""
    
    # Estat√≠sticas f√≠sicas realistas do vento solar
    PHYSICAL_RANGES = {
        "speed": {"min": 200, "max": 2000, "typical": 450},
        "density": {"min": 0.1, "max": 100, "typical": 7},
        "bz_gsm": {"min": -50, "max": 50, "typical": 0},
        "bt": {"min": 0, "max": 100, "typical": 5}
    }
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = HACConfig(config_path)
        paths = self.config.get("paths")
        self.model_dir = paths["model_dir"]
        self.default_targets = self.config.get("targets")["primary"]
        
        # Estado interno otimizado
        self.models: Dict[int, Any] = {}
        self.meta: Dict[int, Dict[str, Any]] = {}
        self.scalers_y: Dict[int, Any] = {}
        self._is_loaded = False
        self._load_attempts = 0
        
        logger.info("üß† HACv6PredictorGithub - Inicializando (GitHub Free Optimized)")
        
    def _load_model_memory_safe(self, model_path: str) -> Optional[Any]:
        """Carrega modelo com controle de mem√≥ria"""
        if not TF_AVAILABLE:
            raise RuntimeError("TensorFlow n√£o est√° dispon√≠vel")
            
        try:
            # Limpeza agressiva de mem√≥ria antes do carregamento
            gc.collect()
            
            # Configura√ß√µes para economizar mem√≥ria
            import tensorflow as tf
            tf.config.set_visible_devices([], 'GPU')  # Desabilita GPU no Codespaces
            
            model = load_model(model_path)
            logger.info(f"‚úÖ Modelo carregado: {os.path.basename(model_path)}")
            return model
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar {model_path}: {e}")
            return None

    def _create_fallback_scaler(self, horizon: int) -> Any:
        """Cria scaler de fallback baseado em estat√≠sticas f√≠sicas realistas"""
        from sklearn.preprocessing import StandardScaler
        
        logger.warning(f"üîÑ Criando scaler de fallback para H{horizon}")
        
        scaler = StandardScaler()
        n_samples = 5000  # Reduzido para economizar mem√≥ria
        
        # Gera dados baseados em ranges f√≠sicos realistas
        speed_data = np.random.normal(450, 150, n_samples)  # 300-600 km/s t√≠pico
        bz_data = np.random.normal(0, 6, n_samples)         # -10 a +10 nT t√≠pico
        density_data = np.random.normal(7, 4, n_samples)    # 3-15 cm‚Åª¬≥ t√≠pico
        
        synthetic_data = np.column_stack([speed_data, bz_data, density_data])
        scaler.fit(synthetic_data)
        
        return scaler

    def _load_all_memory_optimized(self) -> None:
        """Carrega modelos de forma otimizada para mem√≥ria"""
        if not os.path.isdir(self.model_dir):
            logger.warning(f"üìÅ Diret√≥rio n√£o encontrado: {self.model_dir}")
            os.makedirs(self.model_dir, exist_ok=True)
            return

        pattern = re.compile(r"^(?P<type>\w+)_h(?P<h>\d+)_")
        loaded_count = 0

        # Ordena por horizonte para carregar do menor para o maior
        folders = sorted(os.listdir(self.model_dir), 
                        key=lambda x: int(pattern.match(x).group("h")) if pattern.match(x) else 999)
        
        for folder in folders:
            full_path = os.path.join(self.model_dir, folder)
            if not os.path.isdir(full_path):
                continue

            match = pattern.match(folder)
            if not match:
                continue

            horizon = int(match.group("h"))
            model_path = os.path.join(full_path, "model.keras")
            meta_path = os.path.join(full_path, "metadata.json")
            scaler_y_path = os.path.join(full_path, "scaler_Y.pkl")

            if not os.path.exists(model_path):
                logger.warning(f"üì¶ Modelo n√£o encontrado: {model_path}")
                continue

            # Carrega modelo com controle de mem√≥ria
            model = self._load_model_memory_safe(model_path)
            if model is None:
                continue

            # Carrega metadata
            meta = {}
            if os.path.exists(meta_path):
                try:
                    with open(meta_path, "r") as f:
                        meta = json.load(f)
                except Exception as e:
                    logger.warning(f"üìÑ Erro ao carregar metadata {meta_path}: {e}")

            # Carrega ou cria scaler Y
            scaler_y = None
            if os.path.exists(scaler_y_path):
                try:
                    import joblib
                    scaler_y = joblib.load(scaler_y_path)
                    logger.info(f"‚úÖ Scaler Y carregado para H{horizon}")
                except Exception as e:
                    logger.error(f"‚ùå Erro ao carregar scaler Y {scaler_y_path}: {e}")
                    scaler_y = self._create_fallback_scaler(horizon)
            else:
                logger.warning(f"‚ö†Ô∏è Scaler Y n√£o encontrado para H{horizon}, criando fallback")
                scaler_y = self._create_fallback_scaler(horizon)

            self.models[horizon] = model
            self.meta[horizon] = meta
            self.scalers_y[horizon] = scaler_y
            loaded_count += 1
            
            # Limpeza de mem√≥ria entre carregamentos
            if loaded_count % 2 == 0:  # A cada 2 modelos
                gc.collect()

        if loaded_count > 0:
            self._is_loaded = True
            logger.info(f"üéØ {loaded_count} modelos carregados: {sorted(self.models.keys())}")
        else:
            logger.warning("‚ö†Ô∏è Nenhum modelo foi carregado")

    def ensure_loaded(self) -> bool:
        """Garante que os modelos est√£o carregados com fallback"""
        if not self._is_loaded:
            self._load_attempts += 1
            if self._load_attempts > 3:
                logger.error("üí• Muitas tentativas de carregamento - abortando")
                return False
            self._load_all_memory_optimized()
        return self._is_loaded

    def _validate_physical_ranges(self, values: Dict[str, float]) -> Tuple[bool, List[str]]:
        """Valida se os valores est√£o dentro de ranges f√≠sicos poss√≠veis"""
        warnings = []
        valid = True
        
        for param, value in values.items():
            if param in self.PHYSICAL_RANGES:
                range_info = self.PHYSICAL_RANGES[param]
                
                if value < range_info["min"] or value > range_info["max"]:
                    warnings.append(f"{param} fora do range: {value:.1f} (esperado: {range_info['min']}-{range_info['max']})")
                    valid = False
                elif abs(value - range_info["typical"]) > 3 * (range_info["max"] - range_info["min"]) / 4:
                    warnings.append(f"{param} at√≠pico: {value:.1f} (t√≠pico: ~{range_info['typical']})")
        
        return valid, warnings

    def _apply_scaler_safe(self, y_scaled: np.ndarray, horizon: int) -> np.ndarray:
        """Aplica scaler Y com tratamento robusto de erros"""
        try:
            if horizon in self.scalers_y and self.scalers_y[horizon] is not None:
                # Garante formato 2D para inverse_transform
                y_scaled_2d = y_scaled.reshape(1, -1)
                y_real = self.scalers_y[horizon].inverse_transform(y_scaled_2d)[0]
                
                # Log para debug
                logger.debug(f"üîÑ Scaling H{horizon}: {y_scaled} ‚Üí {y_real}")
                return y_real
            else:
                logger.error(f"üö® SCALER Y N√ÉO DISPON√çVEL para H{horizon}")
                # Fallback: assume que j√° est√° desnormalizado (arriscado)
                return y_scaled
                
        except Exception as e:
            logger.error(f"‚ùå Erro no scaling H{horizon}: {e}")
            # Fallback extremo: retorna valores t√≠picos
            return np.array([450.0, 0.0, 7.0])  # Valores t√≠picos

    def predict_safe(self, X_window: np.ndarray, horizon: int) -> PredictionResult:
        """
        Previs√£o com tratamento completo de erros e valida√ß√£o
        
        Args:
            X_window: Array (lookback, n_features) ou (1, lookback, n_features)
            horizon: Horizonte de previs√£o em horas
            
        Returns:
            PredictionResult: Resultado estruturado com valida√ß√µes
        """
        if not self.ensure_loaded():
            raise RuntimeError("Predictor n√£o est√° carregado")
            
        if horizon not in self.models:
            available = sorted(self.models.keys())
            raise ValueError(f"Modelo H{horizon} n√£o encontrado. Dispon√≠veis: {available}")

        try:
            # Preprocessamento seguro da entrada
            arr = np.asarray(X_window, dtype=np.float32)  # float32 para economia
            
            if arr.ndim == 2:
                arr = np.expand_dims(arr, axis=0)
            elif arr.ndim != 3:
                raise ValueError(f"Shape inv√°lido: {arr.shape}. Esperado: (lookback, n_features) ou (1, lookback, n_features)")

            # Previs√£o com batch_size=1 para economia de mem√≥ria
            model = self.models[horizon]
            y_scaled = model.predict(arr, batch_size=1, verbose=0)[0]
            
            # Aplica scaler Y com fallbacks
            y_real = self._apply_scaler_safe(y_scaled, horizon)
            
            # Mapeia para dicion√°rio
            targets = self.meta.get(horizon, {}).get("targets", self.default_targets)
            values = {targets[i]: float(y_real[i]) for i in range(len(targets))}
            
            # Valida√ß√£o f√≠sica
            is_valid, warnings = self._validate_physical_ranges(values)
            
            # Checks individuais por par√¢metro
            physical_checks = {}
            for param in values:
                if param in self.PHYSICAL_RANGES:
                    range_info = self.PHYSICAL_RANGES[param]
                    physical_checks[param] = (
                        range_info["min"] <= values[param] <= range_info["max"]
                    )
            
            # Limpeza de mem√≥ria
            del arr, y_scaled
            gc.collect()
            
            return PredictionResult(
                values=values,
                horizon=horizon,
                valid=is_valid,
                warnings=warnings,
                physical_checks=physical_checks
            )
            
        except Exception as e:
            logger.error(f"üí• Erro na previs√£o H{horizon}: {e}")
            raise

    def predict_from_features_array(self, X_window: np.ndarray, horizon: int) -> Dict[str, float]:
        """Interface compat√≠vel com vers√£o anterior"""
        result = self.predict_safe(X_window, horizon)
        
        # Log warnings se houver
        if result.warnings:
            for warning in result.warnings:
                logger.warning(f"‚ö†Ô∏è H{horizon}: {warning}")
        
        return result.values

    def predict_from_dataframe(self, df_feat: pd.DataFrame, horizon: int, lookback: int = None) -> Dict[str, float]:
        """Previs√£o a partir de DataFrame"""
        if not self.ensure_loaded():
            raise RuntimeError("Predictor n√£o est√° carregado")
            
        # Determina lookback
        if lookback is None:
            lookback = self.meta.get(horizon, {}).get(
                "lookback",
                self.config.get("training")["main_lookback"],
            )

        if len(df_feat) < lookback:
            raise ValueError(
                f"Dados insuficientes: precisa {lookback}, tem {len(df_feat)}"
            )

        # Converte para float32 para economia
        window = df_feat.tail(lookback).astype(np.float32).values
        return self.predict_from_features_array(window, horizon)

    def batch_predict(self, X_windows: List[np.ndarray], horizons: List[int]) -> List[PredictionResult]:
        """Previs√£o em lote otimizada"""
        results = []
        
        for X_window, horizon in zip(X_windows, horizons):
            try:
                result = self.predict_safe(X_window, horizon)
                results.append(result)
            except Exception as e:
                logger.error(f"‚ùå Erro no batch H{horizon}: {e}")
                # Resultado de fallback
                results.append(PredictionResult(
                    values={},
                    horizon=horizon,
                    valid=False,
                    warnings=[f"Erro: {e}"],
                    physical_checks={}
                ))
            
            # Limpeza peri√≥dica de mem√≥ria
            if len(results) % 3 == 0:
                gc.collect()
        
        return results

    def get_status(self) -> Dict[str, Any]:
        """Retorna status completo do predictor"""
        return {
            "loaded": self._is_loaded,
            "models_loaded": len(self.models),
            "scalers_loaded": len([s for s in self.scalers_y.values() if s is not None]),
            "available_horizons": sorted(self.models.keys()),
            "load_attempts": self._load_attempts,
            "memory_optimized": True
        }


# Singleton global para otimiza√ß√£o
_predictor_instance = None

def get_predictor_github(config_path: str = "config.yaml") -> HACv6PredictorGithub:
    """Retorna inst√¢ncia singleton otimizada para GitHub"""
    global _predictor_instance
    if _predictor_instance is None:
        _predictor_instance = HACv6PredictorGithub(config_path)
    return _predictor_instance


# Interface de compatibilidade
def get_predictor(config_path: str = "config.yaml") -> HACv6PredictorGithub:
    """Alias para compatibilidade com scripts existentes"""
    return get_predictor_github(config_path)


if __name__ == "__main__":
    # Teste r√°pido
    print("üß™ Teste r√°pido do HACv6PredictorGithub")
    
    try:
        predictor = get_predictor_github()
        status = predictor.get_status()
        print(f"‚úÖ Status: {status}")
        
        if status["models_loaded"] > 0:
            print("üéØ Predictor pronto para uso no GitHub Free!")
        else:
            print("‚ö†Ô∏è Nenhum modelo carregado - verifique a pasta models/")
            
    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")
