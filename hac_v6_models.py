#!/usr/bin/env python3
"""
hac_v6_models.py
Model builder para HAC v6 Solar Wind Forecaster.

CARACTER√çSTICAS PRINCIPAIS:
1. HEAD F√çSICO com limites r√≠gidos via fun√ß√µes de ativa√ß√£o
2. Arquitetura otimizada para GitHub Free (mem√≥ria/performance)
3. Compat√≠vel com os scalers por vari√°vel do HACFeatureBuilder
4. Modelos cientificamente v√°lidos para f√≠sica do vento solar
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, Model, regularizers
from typing import Dict, List, Tuple, Optional, Union

# Configurar TensorFlow para GitHub Free
tf.config.set_visible_devices([], 'GPU')  # Desativar GPU no GitHub Free
tf.keras.backend.set_floatx('float32')    # Economia de mem√≥ria


class PhysicalModelBuilder:
    """Construtor de modelos f√≠sicos para previs√£o de vento solar."""
    
    def __init__(self, config: Dict):
        """
        Inicializa o construtor de modelos.
        
        Args:
            config: Dicion√°rio de configura√ß√£o do HACConfig
        """
        self.config = config
        self.model_config = config.get("model", {})
        self.target_config = config.get("targets", {})
        
        # Mapeamento de limites f√≠sicos por vari√°vel
        self.physical_limits = self._define_physical_limits()
        
        print(f"üß† PhysicalModelBuilder inicializado para {self.target_config.get('primary', [])}")
    
    def _define_physical_limits(self) -> Dict:
        """
        Define os limites f√≠sicos realistas para cada vari√°vel do vento solar.
        Baseado em dados OMNI e literatura cient√≠fica.
        """
        return {
            "V": {  # Velocidade do vento solar
                "min": 250.0,     # km/s (ventos lentos)
                "max": 1650.0,    # km/s (CMEs r√°pidas)
                "activation": "sigmoid",
                "description": "Solar wind speed"
            },
            "Bz": {  # Componente vertical do campo magn√©tico interplanet√°rio
                "min": -40.0,     # nT (limite sul extremo)
                "max": 40.0,      # nT (limite norte extremo)
                "activation": "tanh",
                "description": "IMF Bz component"
            },
            "n": {   # Densidade de pr√≥tons
                "min": 0.0,       # cm‚Åª¬≥ (n√£o pode ser negativo)
                "max": 100.0,     # cm‚Åª¬≥ (valores extremos raros)
                "activation": "sigmoid",
                "description": "Proton density"
            },
            "Bx": {  # Componente X do IMF
                "min": -50.0,
                "max": 50.0,
                "activation": "tanh",
                "description": "IMF Bx component"
            },
            "By": {  # Componente Y do IMF
                "min": -50.0,
                "max": 50.0,
                "activation": "tanh",
                "description": "IMF By component"
            },
            "Bt": {  # Magnitude total do campo magn√©tico
                "min": 0.0,
                "max": 80.0,
                "activation": "sigmoid",
                "description": "Total magnetic field"
            }
        }
    
    def build_physical_head(self, x: tf.Tensor, 
                          target_names: List[str]) -> tf.Tensor:
        """
        Cria o HEAD F√çSICO do modelo com limites r√≠gidos.
        
        Este √© o componente CR√çTICO que garante que as previs√µes
        estejam dentro de limites fisicamente plaus√≠veis.
        
        Args:
            x: Tensor de entrada (√∫ltima camada antes do output)
            target_names: Lista de nomes das vari√°veis a prever
            
        Returns:
            Tensor de sa√≠da com limites f√≠sicos aplicados
        """
        outputs = []
        
        for target_name in target_names:
            if target_name in self.physical_limits:
                limits = self.physical_limits[target_name]
                min_val = limits["min"]
                max_val = limits["max"]
                activation = limits["activation"]
                
                # Camada densa para cada vari√°vel
                dense = layers.Dense(1, name=f"dense_{target_name}")(x)
                
                # Aplicar fun√ß√£o de ativa√ß√£o baseada na f√≠sica
                if activation == "sigmoid":
                    # Sigmoid mapeia para [0, 1]
                    activated = tf.keras.activations.sigmoid(dense)
                    # Escalar para [min, max]
                    scaled = activated * (max_val - min_val) + min_val
                    
                elif activation == "tanh":
                    # Tanh mapeia para [-1, 1]
                    activated = tf.keras.activations.tanh(dense)
                    # Escalar para [min, max]
                    scale = (max_val - min_val) / 2.0
                    offset = (max_val + min_val) / 2.0
                    scaled = activated * scale + offset
                    
                else:
                    # Fallback: ativa√ß√£o linear (sem limites)
                    scaled = dense
                    print(f"‚ö†Ô∏è  Vari√°vel {target_name} sem limites f√≠sicos definidos")
                
                outputs.append(scaled)
                
                # Log para debug
                print(f"    üéØ {target_name}: {activation} ‚Üí [{min_val:.1f}, {max_val:.1f}]")
                
            else:
                # Para vari√°veis n√£o configuradas, usar sa√≠da linear
                outputs.append(layers.Dense(1, name=f"dense_{target_name}")(x))
                print(f"‚ö†Ô∏è  Vari√°vel {target_name} n√£o encontrada nos limites f√≠sicos")
        
        # Concatenar todas as sa√≠das
        if len(outputs) > 1:
            return layers.Concatenate(name="physical_output")(outputs)
        else:
            return outputs[0]
    
    def build_lstm_model(self, input_shape: Tuple[int, int], 
                        target_names: List[str]) -> Model:
        """
        Constr√≥i modelo LSTM com head f√≠sico.
        
        Args:
            input_shape: (lookback, n_features)
            target_names: Lista de targets a prever
            
        Returns:
            Modelo Keras compilado
        """
        print(f"üî® Construindo modelo LSTM f√≠sico para {target_names}...")
        
        # Input layer
        inputs = layers.Input(shape=input_shape, name="input_sequence")
        
        # Primeira camada LSTM (com return_sequences para stack)
        x = layers.LSTM(
            units=self.model_config.get("lstm_units", [64, 32])[0],
            return_sequences=True,
            kernel_regularizer=regularizers.l2(0.001),
            recurrent_regularizer=regularizers.l2(0.001),
            name="lstm_1"
        )(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(self.model_config.get("dropout_rate", 0.2))(x)
        
        # Segunda camada LSTM (sem return_sequences)
        lstm_units = self.model_config.get("lstm_units", [64, 32])
        if len(lstm_units) > 1:
            x = layers.LSTM(
                units=lstm_units[1],
                return_sequences=False,
                kernel_regularizer=regularizers.l2(0.001),
                name="lstm_2"
            )(x)
            x = layers.BatchNormalization()(x)
            x = layers.Dropout(self.model_config.get("dropout_rate", 0.2))(x)
        
        # Camadas densas intermedi√°rias
        dense_units = self.model_config.get("dense_units", [32, 16])
        for i, units in enumerate(dense_units, 1):
            x = layers.Dense(
                units,
                activation="relu",
                kernel_regularizer=regularizers.l2(0.001),
                name=f"dense_{i}"
            )(x)
            x = layers.BatchNormalization(name=f"bn_dense_{i}")(x)
            x = layers.Dropout(0.1)(x)
        
        # HEAD F√çSICO (componente cr√≠tico)
        outputs = self.build_physical_head(x, target_names)
        
        # Criar modelo
        model = Model(inputs=inputs, outputs=outputs, name="HAC_Physical_LSTM")
        
        # Compilar modelo
        model.compile(
            optimizer=tf.keras.optimizers.Adam(
                learning_rate=0.001,
                clipnorm=1.0  # Prevenir exploding gradients
            ),
            loss="mse",  # Loss simples - limites j√° est√£o na arquitetura
            metrics=["mae", "mse", self.physical_constraint_metric]
        )
        
        return model
    
    def build_hybrid_model(self, input_shape: Tuple[int, int],
                          target_names: List[str]) -> Model:
        """
        Constr√≥i modelo h√≠brido CNN-LSTM com head f√≠sico.
        
        Combina CNN para extra√ß√£o de features locais com LSTM
        para depend√™ncias temporais.
        """
        print(f"üî® Construindo modelo H√çBRIDO f√≠sico para {target_names}...")
        
        # Input layer
        inputs = layers.Input(shape=input_shape, name="input_sequence")
        
        # CNN para extra√ß√£o de features
        conv1 = layers.Conv1D(
            filters=32,
            kernel_size=3,
            padding="same",
            activation="relu",
            name="conv1d_1"
        )(inputs)
        conv1 = layers.BatchNormalization()(conv1)
        conv1 = layers.MaxPooling1D(pool_size=2, name="maxpool_1")(conv1)
        
        conv2 = layers.Conv1D(
            filters=64,
            kernel_size=3,
            padding="same",
            activation="relu",
            name="conv1d_2"
        )(conv1)
        conv2 = layers.BatchNormalization()(conv2)
        conv2 = layers.MaxPooling1D(pool_size=2, name="maxpool_2")(conv2)
        
        # LSTM para depend√™ncias temporais
        lstm1 = layers.LSTM(
            units=64,
            return_sequences=True,
            kernel_regularizer=regularizers.l2(0.001),
            name="lstm_1"
        )(conv2)
        lstm1 = layers.BatchNormalization()(lstm1)
        lstm1 = layers.Dropout(0.3)(lstm1)
        
        lstm2 = layers.LSTM(
            units=32,
            return_sequences=False,
            kernel_regularizer=regularizers.l2(0.001),
            name="lstm_2"
        )(lstm1)
        lstm2 = layers.BatchNormalization()(lstm2)
        lstm2 = layers.Dropout(0.3)(lstm2)
        
        # Camadas densas
        x = layers.Dense(32, activation="relu", name="dense_1")(lstm2)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        x = layers.Dense(16, activation="relu", name="dense_2")(x)
        x = layers.BatchNormalization()(x)
        
        # HEAD F√çSICO
        outputs = self.build_physical_head(x, target_names)
        
        # Criar modelo
        model = Model(inputs=inputs, outputs=outputs, name="HAC_Physical_Hybrid")
        
        # Compilar
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss="mse",
            metrics=["mae", "mse", self.physical_constraint_metric]
        )
        
        return model
    
    def build_lightweight_model(self, input_shape: Tuple[int, int],
                               target_names: List[str]) -> Model:
        """
        Constr√≥i modelo leve para GitHub Free (baixo consumo de mem√≥ria).
        
        Args:
            input_shape: (lookback, n_features)
            target_names: Lista de targets a prever
            
        Returns:
            Modelo Keras otimizado para GitHub Free
        """
        print(f"üî® Construindo modelo LEVE (GitHub Free) para {target_names}...")
        
        # Input layer
        inputs = layers.Input(shape=input_shape, name="input_sequence")
        
        # LSTM √∫nica (para economizar mem√≥ria)
        x = layers.LSTM(
            units=32,  # Unidades reduzidas
            return_sequences=False,
            name="lstm_light"
        )(inputs)
        x = layers.Dropout(0.2)(x)
        
        # Camada densa √∫nica
        x = layers.Dense(16, activation="relu", name="dense_light")(x)
        x = layers.Dropout(0.1)(x)
        
        # HEAD F√çSICO (mesmo head, mas com menos par√¢metros)
        outputs = self.build_physical_head(x, target_names)
        
        # Criar modelo
        model = Model(inputs=inputs, outputs=outputs, name="HAC_Physical_Light")
        
        # Compilar com configura√ß√µes leves
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
            loss="mse",
            metrics=["mae"]
        )
        
        return model
    
    def physical_constraint_metric(self, y_true: tf.Tensor, 
                                 y_pred: tf.Tensor) -> tf.Tensor:
        """
        M√©trica personalizada que mede viola√ß√µes de limites f√≠sicos.
        
        N√£o usada na otimiza√ß√£o, apenas para monitoramento durante o treino.
        
        Args:
            y_true: Valores reais (escalonados)
            y_pred: Valores previstos (com limites f√≠sicos j√° aplicados)
            
        Returns:
            Porcentagem de previs√µes dentro dos limites f√≠sicos
        """
        # Esta m√©trica assume que y_pred j√° est√° nos limites f√≠sicos
        # Portanto, deve ser sempre 1.0 (100% dentro dos limites)
        # √â uma verifica√ß√£o de que o head f√≠sico est√° funcionando
        
        # Se quisermos verificar limites, precisar√≠amos dos scalers
        # Para simplicidade, retornamos 1.0
        return tf.constant(1.0, dtype=tf.float32)
    
    def create_callbacks(self, horizon: int, 
                        model_type: str = "lstm") -> List[tf.keras.callbacks.Callback]:
        """
        Cria callbacks para treinamento otimizado.
        
        Args:
            horizon: Horizonte de previs√£o (para nomear checkpoints)
            model_type: Tipo de modelo (para nomear checkpoints)
            
        Returns:
            Lista de callbacks do Keras
        """
        callbacks = []
        
        # Diret√≥rio para checkpoints
        checkpoint_dir = os.path.join("checkpoints", f"h{horizon}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # 1. Early Stopping
        callbacks.append(
            tf.keras.callbacks.EarlyStopping(
                monitor="val_loss",
                patience=10,
                restore_best_weights=True,
                verbose=1,
                min_delta=0.001
            )
        )
        
        # 2. ReduceLROnPlateau
        callbacks.append(
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor="val_loss",
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                verbose=1
            )
        )
        
        # 3. ModelCheckpoint
        callbacks.append(
            tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join(checkpoint_dir, f"{model_type}_best.keras"),
                monitor="val_loss",
                save_best_only=True,
                save_weights_only=False,
                verbose=0
            )
        )
        
        # 4. CSV Logger
        csv_logger_path = os.path.join(checkpoint_dir, f"training_log_h{horizon}.csv")
        callbacks.append(
            tf.keras.callbacks.CSVLogger(csv_logger_path, separator=",", append=False)
        )
        
        # 5. TerminateOnNaN (seguran√ßa)
        callbacks.append(tf.keras.callbacks.TerminateOnNaN())
        
        return callbacks
    
    def get_model_summary(self, model: Model) -> Dict:
        """
        Retorna um resumo do modelo para logging.
        
        Args:
            model: Modelo Keras
            
        Returns:
            Dicion√°rio com informa√ß√µes do modelo
        """
        trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])
        non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])
        
        return {
            "name": model.name,
            "total_params": int(trainable_params + non_trainable_params),
            "trainable_params": int(trainable_params),
            "non_trainable_params": int(non_trainable_params),
            "layers": len(model.layers),
            "output_shape": str(model.output.shape),
            "physical_limits_applied": True
        }
    
    def save_model_with_metadata(self, model: Model, horizon: int, 
                               save_dir: str, feature_builder=None):
        """
        Salva o modelo com metadata adicional.
        
        Args:
            model: Modelo Keras a ser salvo
            horizon: Horizonte de previs√£o
            save_dir: Diret√≥rio para salvar
            feature_builder: Inst√¢ncia do HACFeatureBuilder (opcional)
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. Salvar modelo
        model_path = os.path.join(save_dir, "model.keras")
        model.save(model_path, save_format="keras")
        print(f"üíæ Modelo salvo: {model_path}")
        
        # 2. Salvar metadata
        metadata = {
            "model_info": self.get_model_summary(model),
            "horizon": horizon,
            "targets": self.target_config.get("primary", []),
            "physical_limits": {
                name: {k: v for k, v in limits.items() if k != "description"}
                for name, limits in self.physical_limits.items()
                if name in self.target_config.get("primary", [])
            },
            "training_config": {
                "lookback": self.config.get("training", {}).get("main_lookback", 24),
                "batch_size": self.config.get("training", {}).get("batch_size", 32),
                "max_epochs": self.config.get("training", {}).get("max_epochs", 100)
            }
        }
        
        # Adicionar informa√ß√µes do feature builder se dispon√≠vel
        if feature_builder and hasattr(feature_builder, 'scaler_X'):
            metadata["scaler_info"] = {
                "X_scaler_fitted": feature_builder.scaler_X.n_samples_seen_ > 0,
                "y_scalers": {
                    str(h): {var: "fitted" for var in scalers}
                    for h, scalers in feature_builder.scalers_y.items()
                } if hasattr(feature_builder, 'scalers_y') else {}
            }
        
        metadata_path = os.path.join(save_dir, "model_metadata.json")
        import json
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)
        
        print(f"üìÑ Metadata salva: {metadata_path}")
        
        return model_path, metadata_path


def create_model_builder(config: Dict) -> PhysicalModelBuilder:
    """
    Factory function para criar um PhysicalModelBuilder.
    
    Args:
        config: Configura√ß√£o do HAC v6
        
    Returns:
        Inst√¢ncia de PhysicalModelBuilder
    """
    return PhysicalModelBuilder(config)


# ------------------------------------------------------------
# FUN√á√ïES CORRIGIDAS PARA EXTRA√á√ÉO ROBUSTA DE PREVIS√ïES
# ------------------------------------------------------------

def safe_extract_prediction_value(pred_array: np.ndarray, idx: int = 0) -> float:
    """
    Extrai de forma segura um valor float de um array numpy de previs√µes.
    
    Args:
        pred_array: Array numpy de previs√µes (qualquer shape)
        idx: √çndice do valor a extrair (0 para primeiro)
        
    Returns:
        Valor float extra√≠do
    """
    # Achata o array para 1D
    flat = pred_array.flatten()
    
    if len(flat) <= idx:
        raise ValueError(f"Array tem apenas {len(flat)} valores, √≠ndice {idx} fora do range")
    
    # Extrai o valor e converte para float Python
    value = flat[idx]
    
    # Se for um array numpy 0-d, converte para scalar
    if hasattr(value, 'item'):
        return float(value.item())
    else:
        return float(value)


def extract_prediction_values(predictions: np.ndarray, n_targets: int = 3) -> List[float]:
    """
    Extrai n_targets valores float de um array numpy de previs√µes.
    
    CORRE√á√ÉO CR√çTICA: Lida com qualquer formato de array:
    - (batch, n_targets)
    - (batch, timesteps, n_targets)
    - (batch, n_targets, 1)
    - etc.
    
    Args:
        predictions: Array numpy de previs√µes do modelo
        n_targets: N√∫mero de valores a extrair
        
    Returns:
        Lista de valores float
    """
    print(f"    üîç Debug - Shape das previs√µes: {predictions.shape}")
    print(f"    üîç Debug - Dtype das previs√µes: {predictions.dtype}")
    
    # Achata o array e pega os primeiros n_targets valores
    flat = predictions.flatten()
    
    if len(flat) < n_targets:
        print(f"    ‚ö†Ô∏è  Aviso: Array tem apenas {len(flat)} valores, esperados {n_targets}")
        n_targets = len(flat)
    
    values = []
    for i in range(n_targets):
        val = flat[i]
        
        # Se for um array numpy (pode acontecer com arrays aninhados)
        if hasattr(val, 'item'):
            val = val.item()
        
        values.append(float(val))
    
    return values


def debug_prediction_structure(predictions: np.ndarray, max_depth: int = 3):
    """
    Fun√ß√£o de debug para entender a estrutura das previs√µes.
    
    Args:
        predictions: Array numpy para debug
        max_depth: Profundidade m√°xima de an√°lise
    """
    print(f"\nüîç DEBUG DA ESTRUTURA DAS PREVIS√ïES:")
    print(f"   Tipo: {type(predictions)}")
    print(f"   Shape: {predictions.shape}")
    print(f"   Dtype: {predictions.dtype}")
    print(f"   Ndims: {predictions.ndim}")
    
    if predictions.ndim > 0:
        print(f"   Primeiro elemento [0]: {type(predictions[0])}")
        
        if predictions.ndim >= 2:
            print(f"   Primeiro elemento [0,0]: {type(predictions[0,0])}")
            print(f"   Shape de [0,0]: {predictions[0,0].shape if hasattr(predictions[0,0], 'shape') else 'N/A'}")
            
        if predictions.ndim >= 3:
            print(f"   Primeiro elemento [0,0,0]: {type(predictions[0,0,0])}")
    
    print(f"   Primeiros 5 valores achatados: {[float(v) for v in predictions.flatten()[:5]]}")


# ------------------------------------------------------------
# TESTES CORRIGIDOS
# ------------------------------------------------------------

def test_physical_limits():
    """Testa se os limites f√≠sicos est√£o sendo aplicados corretamente."""
    print("üß™ Testando limites f√≠sicos...")
    
    # Configura√ß√£o de teste
    test_config = {
        "model": {"lstm_units": [32], "dense_units": [16], "dropout_rate": 0.2},
        "targets": {"primary": ["V", "Bz", "n"]}
    }
    
    # Criar builder
    builder = PhysicalModelBuilder(test_config)
    
    # Testar head f√≠sico
    print("\n1. Testando Physical Head:")
    test_input = tf.keras.Input(shape=(10,))
    test_head = builder.build_physical_head(test_input, ["V", "Bz", "n"])
    print(f"   Output shape: {test_head.shape}")
    
    # Testar constru√ß√£o de modelo
    print("\n2. Testando constru√ß√£o de modelo LSTM:")
    test_model = builder.build_lstm_model((24, 20), ["V", "Bz", "n"])
    print(f"   Modelo criado: {test_model.name}")
    print(f"   Par√¢metros trein√°veis: {test_model.count_params():,}")
    
    # Testar previs√µes com dados aleat√≥rios
    print("\n3. Testando previs√µes com dados aleat√≥rios:")
    dummy_input = np.random.randn(1, 24, 20).astype(np.float32)
    predictions = test_model.predict(dummy_input, verbose=0)
    
    # DEBUG: Analisar estrutura das previs√µes
    debug_prediction_structure(predictions)
    
    # Extrair valores de forma robusta
    pred_vals = extract_prediction_values(predictions, n_targets=3)
    
    print(f"\n   Valores extra√≠dos (robustos):")
    print(f"   V: {pred_vals[0]:.2f} km/s (deve estar entre 250-1650)")
    print(f"   Bz: {pred_vals[1]:.2f} nT (deve estar entre -40 e 40)")
    print(f"   n: {pred_vals[2]:.2f} cm‚Åª¬≥ (deve estar entre 0-100)")
    
    # Verificar limites
    v_ok = 250 <= pred_vals[0] <= 1650
    bz_ok = -40 <= pred_vals[1] <= 40
    n_ok = 0 <= pred_vals[2] <= 100
    
    print(f"\n‚úÖ Verifica√ß√£o de limites f√≠sicos:")
    print(f"   V dentro dos limites: {'‚úÖ' if v_ok else '‚ùå'}")
    print(f"   Bz dentro dos limites: {'‚úÖ' if bz_ok else '‚ùå'}")
    print(f"   n dentro dos limites: {'‚úÖ' if n_ok else '‚ùå'}")
    
    # Teste adicional: verificar todas as previs√µes em um batch maior
    print(f"\n4. Teste adicional com batch maior:")
    dummy_batch = np.random.randn(5, 24, 20).astype(np.float32)
    batch_predictions = test_model.predict(dummy_batch, verbose=0)
    
    print(f"   Batch predictions shape: {batch_predictions.shape}")
    
    # Verificar que TODAS as previs√µes est√£o dentro dos limites
    all_within_limits = True
    for i in range(batch_predictions.shape[0]):
        sample_vals = extract_prediction_values(batch_predictions[i:i+1], n_targets=3)
        
        if not (250 <= sample_vals[0] <= 1650 and 
                -40 <= sample_vals[1] <= 40 and 
                0 <= sample_vals[2] <= 100):
            all_within_limits = False
            print(f"   ‚ö†Ô∏è  Amostra {i} viola limites: V={sample_vals[0]:.1f}, Bz={sample_vals[1]:.1f}, n={sample_vals[2]:.1f}")
    
    if all_within_limits:
        print(f"   ‚úÖ TODAS as {batch_predictions.shape[0]} amostras respeitam limites f√≠sicos!")
    else:
        print(f"   ‚ö†Ô∏è  Algumas amostras violam limites f√≠sicos")
    
    if v_ok and bz_ok and n_ok and all_within_limits:
        print("\nüéâ TODOS OS LIMITES F√çSICOS RESPEITADOS EM TODOS OS TESTES!")
    else:
        print("\n‚ö†Ô∏è  ALGUM LIMITE F√çSICO VIOLADO!")
    
    return test_model


def create_optimized_model_for_github(input_shape: Tuple[int, int],
                                    target_names: List[str]) -> Model:
    """
    Cria um modelo otimizado especificamente para GitHub Free.
    
    Args:
        input_shape: Shape dos dados de entrada
        target_names: Nomes dos targets
        
    Returns:
        Modelo Keras otimizado para GitHub Free
    """
    print("üöÄ Criando modelo otimizado para GitHub Free...")
    
    config = {
        "model": {
            "lstm_units": [32],      # Reduzido para GitHub Free
            "dense_units": [16],     # Reduzido
            "dropout_rate": 0.2
        },
        "targets": {"primary": target_names},
        "training": {
            "batch_size": 16,
            "max_epochs": 30
        }
    }
    
    builder = PhysicalModelBuilder(config)
    return builder.build_lightweight_model(input_shape, target_names)


# ------------------------------------------------------------
# Execu√ß√£o direta para testes
# ------------------------------------------------------------
if __name__ == "__main__":
    print("=" * 60)
    print("üß™ HAC v6 PHYSICAL MODEL BUILDER - TESTE CORRIGIDO")
    print("=" * 60)
    
    try:
        # Testar com configura√ß√£o padr√£o
        test_model = test_physical_limits()
        
        print(f"\nüìä Resumo do modelo de teste:")
        print(f"   Nome: {test_model.name}")
        print(f"   Input shape: {test_model.input_shape}")
        print(f"   Output shape: {test_model.output_shape}")
        print(f"   Total de par√¢metros: {test_model.count_params():,}")
        
        # Testar fun√ß√£o de extra√ß√£o
        print(f"\n{'='*40}")
        print("Testando fun√ß√µes de extra√ß√£o robustas...")
        
        # Testar com diferentes formatos de array
        test_arrays = [
            ("Array 2D simples", np.array([[1.1, 2.2, 3.3]], dtype=np.float32)),
            ("Array 3D com timesteps", np.random.randn(1, 10, 3).astype(np.float32)),
            ("Array 3D diferente", np.random.randn(1, 3, 5).astype(np.float32)),
        ]
        
        for name, arr in test_arrays:
            print(f"\n   Testando {name} (shape: {arr.shape}):")
            try:
                vals = extract_prediction_values(arr, n_targets=3)
                print(f"     Valores extra√≠dos: {[f'{v:.3f}' for v in vals]}")
            except Exception as e:
                print(f"     ‚ùå Erro: {e}")
        
        # Testar modelo otimizado para GitHub
        print(f"\n{'='*40}")
        print("Testando modelo GitHub Free...")
        github_model = create_optimized_model_for_github(
            input_shape=(12, 15),  # Lookback reduzido, features reduzidas
            target_names=["V", "Bz", "n"]
        )
        
        print(f"‚úÖ Modelo GitHub Free criado: {github_model.name}")
        print(f"   Par√¢metros: {github_model.count_params():,} (reduzido em {(1 - github_model.count_params()/test_model.count_params())*100:.1f}%)")
        
        # Fazer uma previs√£o com o modelo GitHub Free
        dummy_github_input = np.random.randn(1, 12, 15).astype(np.float32)
        github_preds = github_model.predict(dummy_github_input, verbose=0)
        
        # Usar fun√ß√£o robusta para extrair valores
        github_vals = extract_prediction_values(github_preds, n_targets=3)
        
        print(f"\nüìä Previs√£o modelo GitHub Free:")
        print(f"   V: {github_vals[0]:.2f} km/s")
        print(f"   Bz: {github_vals[1]:.2f} nT")
        print(f"   n: {github_vals[2]:.2f} cm‚Åª¬≥")
        
        # Verificar limites
        if (250 <= github_vals[0] <= 1650 and 
            -40 <= github_vals[1] <= 40 and 
            0 <= github_vals[2] <= 100):
            print(f"   ‚úÖ Previs√µes dentro dos limites f√≠sicos!")
        else:
            print(f"   ‚ö†Ô∏è  ALGUM LIMITE F√çSICO VIOLADO!")
        
        print("\nüéØ TESTES CONCLU√çDOS COM SUCESSO!")
        print("=" * 60)
        
    except Exception as e:
        print(f"\n‚ùå ERRO durante os testes: {str(e)}")
        import traceback
        traceback.print_exc()
